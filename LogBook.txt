2022_08_17_v0
    * version zero to test the workflow
    * no important stuff here


2022_08_20_v1
    * uses L1TauMinatorNtuples/v0 produced on the 20th of August
    * first try at training with the following input:
        X is (None, N, M, 5)
            N runs over eta, M runs over phi
            5 features are: Ieta, Iphi, Iem, Ihad, EgIet
                        
        Y is (None, 3)
            3 targets are: hwVisPt, hwVisEmPt, hwVisHadPt

    * used the CNNs tagged as _5channels2CNNs and _5channels1CNN
    * two separate CNNs for EM and HAD part with targets the hwVisEmPt, hwVisHadPt
        -> after discussion with Shamik, this is not corect: a particle that at gen level is em (e.g. pi0) might give had deposit too ; and viceversa a had particle (e.g. pi+) can give em deposit
        -> the fact that I am using a CNN makes had and em parts be mixed togetehr at training, so I can just regress on the genPt without having to split em/had parts
    * the CNNs are also getting as input eta and phi 
        -> after discussion with Shamik, this is not useful: the CNN will see a picture of a channel which is smooth and with a linear gradient, there is nothing that can be learnt from there
        -> it is best to move the eta/phi information to the Dense layer of the NN
    * the CNNs are made with 2D convolutions
        -> after discussion with Shamik, could possibly go to Conv3D but will have more paraeters and more resources will be needed in FPGA (bad!!)


2022_08_23_v2
    * uses L1TauMinatorNtuples/v0 produced on the 20th of August
    * implement all the comments received from Shamik and reported above
    * tested training and other stuff to make sure all works fine
    * done only for 9x9 clusters, will move to L1TauMinatorNtuples/v1


2022_08_25_v3
    * full blown training for all possible shapes
    * identification works very well
    * calibration has some issues and gives very good rms but very bad mean
    * 5x9 looks to be the best working option for identification
    * here the input for the calibration was still the iet deposit -> had to manually modif y the targets

2022_08_27_v4
    * all inputs fixed
    * training only for 5x9 clusters
    * identification works really well
    * calibration still shows the bad mean behaviour
        * tried multiple different losses
        * problem looks to lay in the tf.nn.l2_loss() function
        * all built-in losses work much better, MeanAbsolutePercentageError() gives the best results (MAPE)

2022_08_29_v5
    * modify position inputs for the cluster seed
        -> pass from OHE ieta/iphi to the float value of ieta/iphi
    * correct the input taus cut to be on vis_pt and not on pt
    * tested DNN only calibration
        -> gives exactly the same performance as the CNN+DNN 
        -> this confirms the possibility of having the same CNN for both Ident/Calib and then separate DNN after the CNN
    * tested efficiency 
        -> the current problem is the calibration which lowers the efficiency even wothout applying the identification
        -> possible solution: run till the rate estimation and see what happens
        -> real only solution: need to have a much better calibration
    * tested barrel only calibration and identification
        -> the results improvement is almost none
        -> should be able to easily go on running with the full detector as an input
    * tested VBF only training
        -> yield better performance because huge outlayers from Z' dataset are removed
        -> possibly remove the Z' datasets from the tarining and maybe just use them for some additional validation

    * in the tensirization, the cut on eta here was not working properly because no absolute value was applied!! AHRG!!

2022_08_30_v6
    * fixed eta cut bug in tensorization
    * using only VBF samples (GGHH also available if wanted)
    * first work up until the rate estimation
    * barrel only algorithm

2022_08_31_v7
    * introduced BDTs for HGCAL
    * CNN calibration standard in the barrel
    * BDT calibration standard in the endcap





