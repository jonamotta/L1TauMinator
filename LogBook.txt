2022_08_17_v0
    * version zero to test the workflow
    * no important stuff here


2022_08_20_v1
    * uses L1TauMinatorNtuples/v0 produced on the 20th of August
    * first try at training with the following input:
        X is (None, N, M, 5)
            N runs over eta, M runs over phi
            5 features are: Ieta, Iphi, Iem, Ihad, EgIet
                        
        Y is (None, 3)
            3 targets are: hwVisPt, hwVisEmPt, hwVisHadPt

    * used the CNNs tagged as _5channels2CNNs and _5channels1CNN
    * two separate CNNs for EM and HAD part with targets the hwVisEmPt, hwVisHadPt
        -> after discussion with Shamik, this is not corect: a particle that at gen level is em (e.g. pi0) might give had deposit too ; and viceversa a had particle (e.g. pi+) can give em deposit
        -> the fact that I am using a CNN makes had and em parts be mixed togetehr at training, so I can just regress on the genPt without having to split em/had parts
    * the CNNs are also getting as input eta and phi 
        -> after discussion with Shamik, this is not useful: the CNN will see a picture of a channel which is smooth and with a linear gradient, there is nothing that can be learnt from there
        -> it is best to move the eta/phi information to the Dense layer of the NN
    * the CNNs are made with 2D convolutions
        -> after discussion with Shamik, could possibly go to Conv3D but will have more paraeters and more resources will be needed in FPGA (bad!!)


2022_08_23_v2
    * uses L1TauMinatorNtuples/v0 produced on the 20th of August
    * implement all the comments received from Shamik and reported above
    * tested training and other stuff to make sure all works fine
    * done only for 9x9 clusters, will move to L1TauMinatorNtuples/v1


2022_08_25_v3
    * full blown training for all possible shapes
    * identification works very well
    * calibration has some issues and gives very good rms but very bad mean
    * 5x9 looks to be the best working option for identification
    * here the input for the calibration was still the iet deposit -> had to manually modif y the targets

2022_08_27_v4
    * all inputs fixed
    * training only for 5x9 clusters
    * identification works really well
    * calibration still shows the bad mean behaviour
        * tried multiple different losses
        * problem looks to lay in the tf.nn.l2_loss() function
        * all built-in losses work much better, MeanAbsolutePercentageError() gives the best results (MAPE)

2022_08_29_v5
    * modify position inputs for the cluster seed
        -> pass from OHE ieta/iphi to the float value of ieta/iphi
    * correct the input taus cut to be on vis_pt and not on pt
    * tested DNN only calibration
        -> gives exactly the same performance as the CNN+DNN 
        -> this confirms the possibility of having the same CNN for both Ident/Calib and then separate DNN after the CNN
    * tested efficiency 
        -> the current problem is the calibration which lowers the efficiency even wothout applying the identification
        -> possible solution: run till the rate estimation and see what happens
        -> real only solution: need to have a much better calibration
    * tested barrel only calibration and identification
        -> the results improvement is almost none
        -> should be able to easily go on running with the full detector as an input
    * tested VBF only training
        -> yield better performance because huge outlayers from Z' dataset are removed
        -> possibly remove the Z' datasets from the tarining and maybe just use them for some additional validation

    * in the tensirization, the cut on eta here was not working properly because no absolute value was applied!! AHRG!!

2022_08_30_v6
    * fixed eta cut bug in tensorization
    * using only VBF samples (GGHH also available if wanted)
    * first work up until the rate estimation
    * barrel only algorithm

2022_08_31_v7
    * introduced BDTs for HGCAL
    * CNN calibration standard in the barrel
    * BDT calibration standard in the endcap
    * pruning is now operational 
        * calibrator good gives results with 50%/60% of zeros, a bitless good with 75%
        * identifier good gives results with 50%/75% of zeros (will try to push it the furthest possible)

2022_09_10_v8
    * restructuring of the input/output folders to improve workability
    * first try to go up to the full eta coverage TauMinator algo (use event-eta-pt to match taus between CNN and BDT)
    * full work up to quantization
    * full work up to rate estimation
    * still 2 seoarate CNNs for identification and calibration
    
2022_09_14_v9
    * first try to have one single CNN shared between identification and calibration
        * successful, very slight loss in performance of calibration -> should recover that in some other way
    * first try at making CoTraining of the calibrator and the identifier
        * identifier benefits form the presence of the calibration loss
        * calibrator highly degraded from the presence of the identifier loss

    * first time all the way to HLS conversion and resources estimate
        * architechture with:
            - ident: Conv2D(16) + Conv2D(24) + Dense(32) + Dense(16) + Dense(1)
            - calib: Dense(32) + Dense(16) + Dense(1) 
        * resources through the roof
            - CNN: ~5% of vu13p
            - DNN ident: ~1.5% of vu13p
            - DNN calib: ~1.5% of vu13p
        * decided to move to a smaller architechture

2022_09_27_v10
    * move to a smaller architechture to reduce resources usage (choise made on trial and error dimensions)
        - ident: Conv2D(4) + Conv2D(8) + Dense(16) + Dense(8) + Dense(1)
        - calib: Dense(16) + Dense(8) + Dense(1)
    * no HLS ran here

2022_09_28_v11
    * move to quantized input
        - quantization in steps of 0.25GeV
        - max 127.25 GeV
    * HLS only for the spefic model targeted
    * tested ParallelizationFactor for first layer -> ParallelizationFactor=4
        - latency reduced by x2
        - interval reduced by x3
        - resources increased by 10%
        - this means: for ParallelizationFactor=1 to process the same amount of clusters, in the same time of the one with ParallelizationFactor=4, it would need 3 copies
                      --> the parallelization factor 4 gives a 10% increase in resources but that amounts to ~64% of resources saving when considering the multiplicity of clisters to process








