L1TauMinatorNtuples

    v0
        * basic first iteration
        * mainly for workflow testing purpose
        * only has 9x9 clusters

    v1
        * true good first iteration
        * produces all dimensons of clusters: 9x9, 7x7, 5x5, 5x9, 5x7, 3x7, 3x5
        * suffers from many missing HGCAL towers
            - JB says it is due to how TTs are produced : sum modules and use geometry module --> loose TT ith no direct module correspondence 

    v2
        * final version of the ntuples
        * build CLTW clusters based on TTs ordered by pT over the whole coverage

    v2.1
        * first TauProducer like production
        * buggy in the endcap due to a wrong absolute value

    v2.2
        * second TauProducer like production
        * fxed issue with the absolute value
        * efficiency in the endcap is very bad when requiring the geometrical matching of clusters

    v3
        * new ntuple version
            -> same as v2 + cl3d puId + endcap clusters matching + phi-flipping
        * new Phase2Fall22 125X MC samples

    v4
        * new ntuple version
        * Phase2Fall22 125X MC samples
        * implemented new definition of hadronic taus following the Menu Tools approach
        * intorduce eta restriction at clustering time


**************************************************************
**************************************************************


TauMinator Trainings 
    
    2022_08_17_v0
        * version zero to test the workflow
        * no important stuff here


    2022_08_20_v1
        * uses L1TauMinatorNtuples/v0 produced on the 20th of August
        * first try at training with the following input:
            X is (None, N, M, 5)
                N runs over eta, M runs over phi
                5 features are: Ieta, Iphi, Iem, Ihad, EgIet
                            
            Y is (None, 3)
                3 targets are: hwVisPt, hwVisEmPt, hwVisHadPt

        * used the CNNs tagged as _5channels2CNNs and _5channels1CNN
        * two separate CNNs for EM and HAD part with targets the hwVisEmPt, hwVisHadPt
            -> after discussion with Shamik, this is not corect: a particle that at gen level is em (e.g. pi0) might give had deposit too ; and viceversa a had particle (e.g. pi+) can give em deposit
            -> the fact that I am using a CNN makes had and em parts be mixed togetehr at training, so I can just regress on the genPt without having to split em/had parts
        * the CNNs are also getting as input eta and phi 
            -> after discussion with Shamik, this is not useful: the CNN will see a picture of a channel which is smooth and with a linear gradient, there is nothing that can be learnt from there
            -> it is best to move the eta/phi information to the Dense layer of the NN
        * the CNNs are made with 2D convolutions
            -> after discussion with Shamik, could possibly go to Conv3D but will have more paraeters and more resources will be needed in FPGA (bad!!)


    2022_08_23_v2
        * uses L1TauMinatorNtuples/v0 produced on the 20th of August
        * implement all the comments received from Shamik and reported above
        * tested training and other stuff to make sure all works fine
        * done only for 9x9 clusters, will move to L1TauMinatorNtuples/v1


    2022_08_25_v3
        * uses L1TauMinatorNtuples/v1 produced on the 25th of August
        * full blown training for all possible shapes
        * identification works very well
        * calibration has some issues and gives very good rms but very bad mean
        * 5x9 looks to be the best working option for identification
        * here the input for the calibration was still the iet deposit -> had to manually modif y the targets

    2022_08_27_v4
        * uses L1TauMinatorNtuples/v1 produced on the 25th of August
        * all inputs fixed
        * training only for 5x9 clusters
        * identification works really well
        * calibration still shows the bad mean behaviour
            * tried multiple different losses
            * problem looks to lay in the tf.nn.l2_loss() function
            * all built-in losses work much better, MeanAbsolutePercentageError() gives the best results (MAPE)

    2022_08_29_v5
        * uses L1TauMinatorNtuples/v1 produced on the 25th of August
        * modify position inputs for the cluster seed
            -> pass from OHE ieta/iphi to the float value of ieta/iphi
        * correct the input taus cut to be on vis_pt and not on pt
        * tested DNN only calibration
            -> gives exactly the same performance as the CNN+DNN 
            -> this confirms the possibility of having the same CNN for both Ident/Calib and then separate DNN after the CNN
        * tested efficiency 
            -> the current problem is the calibration which lowers the efficiency even wothout applying the identification
            -> possible solution: run till the rate estimation and see what happens
            -> real only solution: need to have a much better calibration
        * tested barrel only calibration and identification
            -> the results improvement is almost none
            -> should be able to easily go on running with the full detector as an input
        * tested VBF only training
            -> yield better performance because huge outlayers from Z' dataset are removed
            -> possibly remove the Z' datasets from the tarining and maybe just use them for some additional validation

        * in the tensirization, the cut on eta here was not working properly because no absolute value was applied!! AHRG!!

    2022_08_30_v6
        * uses L1TauMinatorNtuples/v1 produced on the 25th of August
        * fixed eta cut bug in tensorization
        * using only VBF samples (GGHH also available if wanted)
        * first work up until the rate estimation
        * barrel only algorithm

    2022_08_31_v7
        * uses L1TauMinatorNtuples/v1 produced on the 25th of August
        * introduced BDTs for HGCAL
        * CNN calibration standard in the barrel
        * BDT calibration standard in the endcap
        * pruning is now operational 
            * calibrator good gives results with 50%/60% of zeros, a bitless good with 75%
            * identifier good gives results with 50%/75% of zeros (will try to push it the furthest possible)

    2022_09_10_v8
        * uses L1TauMinatorNtuples/v1 produced on the 25th of August
        * restructuring of the input/output folders to improve workability
        * first try to go up to the full eta coverage TauMinator algo (use event-eta-pt to match taus between CNN and BDT)
        * full work up to quantization
        * full work up to rate estimation
        * still 2 seoarate CNNs for identification and calibration
        
    2022_09_14_v9
        * uses L1TauMinatorNtuples/v1 produced on the 25th of August
        * first try to have one single CNN shared between identification and calibration
            * successful, very slight loss in performance of calibration -> should recover that in some other way
        * first try at making CoTraining of the calibrator and the identifier
            * identifier benefits form the presence of the calibration loss
            * calibrator highly degraded from the presence of the identifier loss

        * first time all the way to HLS conversion and resources estimate
            * architechture with:
                - ident: Conv2D(16) + Conv2D(24) + Dense(32) + Dense(16) + Dense(1)
                - calib: Dense(32) + Dense(16) + Dense(1) 
            * resources through the roof
                - CNN: ~5% of vu13p
                - DNN ident: ~1.5% of vu13p
                - DNN calib: ~1.5% of vu13p
            * decided to move to a smaller architechture

    2022_09_27_v10
        * uses L1TauMinatorNtuples/v1 produced on the 25th of August
        * move to a smaller architechture to reduce resources usage (choise made on trial and error dimensions)
            - ident: Conv2D(4) + Conv2D(8) + Dense(16) + Dense(8) + Dense(1)
            - calib: Dense(16) + Dense(8) + Dense(1)
        * no HLS ran here

    2022_09_28_v11
        * uses L1TauMinatorNtuples/v1 produced on the 25th of August
        * move to quantized input
            - quantization in steps of 0.25GeV
            - max 127.25 GeV
        * HLS only for the spefic model targeted
        * tested ParallelizationFactor for first layer -> ParallelizationFactor=4
            - latency reduced by x2
            - interval reduced by x3
            - resources increased by 10%
            - this means: for ParallelizationFactor=1 to process the same amount of clusters, in the same time of the one with ParallelizationFactor=4, it would need 3 copies
                          --> the parallelization factor 4 gives a 10% increase in resources but that amounts to ~64% of resources saving when considering the multiplicity of clisters to process


    *****************************************************************
    SOLVED PROBLEM WITH THE MISSING TRIGGER TOWERS OF HGCAL
    THE FIX SOLVES ALSO THE PROBLEM WITH THE DEFAULTED BARREL TOWERS
    NO MORE OUT OF SHAPE TOWERCLUTERS AFTER THE FIX
    *****************************************************************


    2022_10_04_v12
        * uses L1TauMinatorNtuples/v2 produced on the 4th of October
        * re-run all the workflow with the newer Ntuples
        * quantization of the inputs to 10 bits as forseen for towers
        * quantization of all HGCAL inputs
        * first move to DNNs for the CL3D part
            - allows decoupling of id from cl3d_pt
            - has drowback of neading feature pre-processing
            - performance is actually better than BDTs
        * re-run all the HLS machinery too
        * make first HLS conversion of the DNNs for CL3Ds
        * move to CMSSW-based evaluation of final performance
            - have a producer running the Tensorflow models
            - run turnons and rate code in the same fashion as the Run3 code
        * first evaluation of performance with the Run3-like code
            - uses the v2.2 ntuples ofr the evaluation
            - performance of the overal algo looks goo 
            BUT
            - the geometrical matching in the endcap is killing completely the performance

    2022_10_18_v13
        * uses L1TauMinatorNtuples/v2 produced on the 4th of October
        * first attempt at having matching of cl3d-cltw matching before everything

    >--------- from here move to Tauminator 2.0 ---------<

    2023_05_15_v14
        * uses L1TauMinatorNtuples/v3 produced on the 15th of May
        * first training with all the new specs:
            - 125X MC
            - cl3d-cltw matching before everything
            - hgcal pu bdt cut on clusters
            - phi-flipping augmentation of the training dataset
        * 

    2023_05_15_v14p1
        * same as 2023_05_15_v14 but using seeding of cltw at 3.0 GeV
        * use the same tensor datasets as 2023_05_15_v14
        * use the same cl3d scaler as 2023_05_15_v14

    2023_05_24_v15
        * prepare models for CMSSW using the optimisation done in 2023_05_15_v14 and 2023_05_15_v14p1
        * use the same tensor datasets as 2023_05_15_v14 and 2023_05_15_v14p1
        * use the same cl3d scaler as 2023_05_15_v14 and 2023_05_15_v14p1
        * use tau producer in emulator style for the performance evaluation
        * evaluate performance
        * evaluate rate

    2023_05_28_v16
        * same models as for v15 BUT quantized and pruned
        * HLS implementation with hls4ml
        * deployment on vu9p using hls4ml accelerator backend

    2023_06_21_v17
        * uses L1TauMinatorNtuples/v4 produced on the 15th of June
            -> this increases a bit the training statistics
            -> this make the definition of hadronic objects the same as in the Menu
        * keep all the rest as for v15/v16 which are the first official version that entered CMSSW
            -> CB CE split = 1.5
            -> clustering eta restrinction 3.0
        
    2023_06_21_v17p1
        * uses L1TauMinatorNtuples/v4 produced on the 15th of June
        * following eta choices:
            -> CB CE split = 1.55
            -> clustering eta restrinction 3.0

    2023_06_21_v17p2
        * uses L1TauMinatorNtuples/v4 produced on the 15th of June
        * following eta choices:
            -> CB CE split = 1.61
            -> clustering eta restrinction 3.0
    
    2023_06_21_v18
        * uses L1TauMinatorNtuples/v4 produced on the 15th of June
        * following eta choices:
            -> CB CE split = 1.5
            -> clustering eta restrinction 2.4

    2023_06_21_v18p1
        * uses L1TauMinatorNtuples/v4 produced on the 15th of June
        * following eta choices:
            -> CB CE split = 1.55
            -> clustering eta restrinction 2.4

    2023_06_21_v18p2
        * uses L1TauMinatorNtuples/v4 produced on the 15th of June
        * following eta choices:
            -> CB CE split = 1.61
            -> clustering eta restrinction 2.4

    2023_06_26_v19
    2023_06_28_v20
        * various tests

    >--------- from here move to Tauminator 3.0 ---------<

    2023_07_04_v21
        













